{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e4ed50b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataset size: 33894\n",
      "validation dataset size: 3766\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from tensorflow.keras.models import load_model\n",
    "import keras_tuner as kt\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.layers import RandomZoom\n",
    "import logging\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "\n",
    "csv_file = r'F:\\Dataset.csv'  # Write the CSV file path\n",
    "image_dir = r'F:\\Labesni\\DataSet\\images'  # Write the images file path\n",
    "\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "file_paths = df['id'].values\n",
    "labels = df[['gender', 'masterCategory', 'subCategory', 'articleType', 'baseColour','season', 'usage']]\n",
    "labels = labels.fillna('Unknown')\n",
    "\n",
    "# One-hot encode labels\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "labels_encoded = encoder.fit_transform(labels)\n",
    "\n",
    "encoded_labels = tf.convert_to_tensor(labels_encoded, dtype=tf.float32)\n",
    "file_paths = [str(file_name) for file_name in file_paths]\n",
    "file_paths = [os.path.join(image_dir, file_name + '.jpg') for file_name in file_paths]\n",
    "\n",
    "# Define number of classes for each category\n",
    "num_classes = {\n",
    "    'gender': 5,\n",
    "    'masterCategory': 3,\n",
    "    'subCategory': 17,\n",
    "    'articleType': 68,\n",
    "    'baseColour': 46,\n",
    "    'season':2,\n",
    "    'usage': 6\n",
    "}\n",
    "\n",
    "# Split encoded labels\n",
    "gender_labels = labels_encoded[:, :num_classes['gender']]\n",
    "masterCategory_labels = labels_encoded[:, num_classes['gender']:num_classes['gender'] + num_classes['masterCategory']]\n",
    "subCategory_labels = labels_encoded[:, num_classes['gender'] + num_classes['masterCategory']:num_classes['gender'] + num_classes['masterCategory'] + num_classes['subCategory']]\n",
    "articleType_labels = labels_encoded[:, num_classes['gender'] + num_classes['masterCategory'] + num_classes['subCategory']:num_classes['gender'] + num_classes['masterCategory'] + num_classes['subCategory'] + num_classes['articleType']]\n",
    "baseColour_labels = labels_encoded[:, num_classes['gender'] + num_classes['masterCategory'] + num_classes['subCategory'] + num_classes['articleType']:num_classes['gender'] + num_classes['masterCategory'] + num_classes['subCategory'] + num_classes['articleType'] + num_classes['baseColour']]\n",
    "season_labels = labels_encoded[:, num_classes['gender'] + num_classes['masterCategory'] + num_classes['subCategory'] + num_classes['articleType'] + num_classes['baseColour']:num_classes['gender'] + num_classes['masterCategory'] + num_classes['subCategory'] + num_classes['articleType'] + num_classes['baseColour'] + num_classes['season']]\n",
    "usage_labels = labels_encoded[:, -num_classes['usage']:]\n",
    "\n",
    "# Image preprocessing functions\n",
    "    \n",
    "def load_and_preprocess_image(file_path):\n",
    "    file_path = file_path.numpy().decode('utf-8')\n",
    "    if not tf.io.gfile.exists(file_path):\n",
    "        return tf.zeros([224, 224, 3])\n",
    "    image = tf.io.read_file(file_path)\n",
    "    try:\n",
    "        image = tf.image.decode_jpeg(image, channels=3)\n",
    "    except:\n",
    "        # Handle decoding errors\n",
    "        return tf.zeros([224, 224, 3])\n",
    "    image = tf.image.resize(image, [224, 224])\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    return image\n",
    "\n",
    "\n",
    "random_zoom = tf.keras.layers.RandomZoom(height_factor=(-0.2, 0.2), width_factor=(-0.2, 0.2)) \n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((file_paths, gender_labels, masterCategory_labels, \n",
    "                                              subCategory_labels, articleType_labels, \n",
    "                                              baseColour_labels, season_labels, usage_labels))\n",
    "\n",
    "\n",
    "def preprocess_image(file_path, gender_label, masterCategory_label, subCategory_label, articleType_label, baseColour_label, season_label, usage_label):\n",
    "    try:\n",
    "        file_path = tf.strings.as_string(file_path)\n",
    "        image = tf.py_function(func=load_and_preprocess_image, inp=[file_path], Tout=tf.float32)\n",
    "        image.set_shape([224, 224, 3])\n",
    "        \n",
    "        # Data Augmentation techniques\n",
    "        image = tf.image.random_flip_left_right(image)\n",
    "        image = tf.image.random_brightness(image, max_delta=0.1)\n",
    "        image = tf.image.random_contrast(image, lower=0.8, upper=1.2)\n",
    "        image = tf.image.random_saturation(image, lower=0.8, upper=1.2)\n",
    "        image = tf.image.random_hue(image, max_delta=0.02)\n",
    "        image = tf.image.random_jpeg_quality(image, min_jpeg_quality=75, max_jpeg_quality=100)\n",
    "        image = random_zoom(tf.expand_dims(image, 0)) \n",
    "        image = tf.squeeze(image, axis=0)\n",
    "\n",
    "        return image, (gender_label, masterCategory_label, subCategory_label, articleType_label, baseColour_label, season_label, usage_label)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error during image preprocessing: {e}\")\n",
    "                \n",
    "def filter_invalid(image, *labels):\n",
    "    zero_tensor = tf.zeros_like(image)\n",
    "    \n",
    "    image_valid = tf.reduce_any(tf.not_equal(image, zero_tensor))\n",
    "    \n",
    "    labels_valid = tf.reduce_any([tf.reduce_any(tf.not_equal(label, tf.zeros_like(label))) for label in labels])\n",
    "    \n",
    "    return tf.logical_and(image_valid, labels_valid)\n",
    "\n",
    "dataset = dataset.filter(lambda image, *labels: filter_invalid(image, *labels))\n",
    "       \n",
    "dataset = dataset.map(lambda x, y1, y2, y3, y4, y5, y6, y7: preprocess_image(x, y1, y2, y3, y4, y5, y6, y7), \n",
    "                      num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "dataset = dataset.shuffle(buffer_size=len(file_paths))\n",
    "\n",
    "total_size = len(file_paths)\n",
    "train_size = int(total_size * 0.9)\n",
    "val_size = total_size - train_size\n",
    "\n",
    "train_dataset = dataset.take(train_size)\n",
    "val_dataset = dataset.skip(train_size)\n",
    "\n",
    "train_dataset = train_dataset.batch(batch_size).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "val_dataset = val_dataset.batch(batch_size).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "steps_per_epoch = train_size // batch_size\n",
    "validation_steps = val_size // batch_size\n",
    "\n",
    "\n",
    "print(f\"train dataset size: {train_size}\")\n",
    "print(f\"validation dataset size: {val_size}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d635850b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation dataset size: 118\n"
     ]
    }
   ],
   "source": [
    "val_dataset_size = len(list(val_dataset))\n",
    "print(f\"Validation dataset size: {val_dataset_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cce27e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "    base_model.trainable = False  # Freeze the base model\n",
    "\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "    # Tune number of units in dense layer\n",
    "    units = hp.Int('units', min_value=256, max_value=1024, step=128)\n",
    "    x = Dense(units, activation='relu')(x)\n",
    "    \n",
    "    # Outputs for multi-label classification\n",
    "    gender_output = Dense(num_classes['gender'], activation='softmax', name='gender')(x)\n",
    "    masterCategory_output = Dense(num_classes['masterCategory'], activation='softmax', name='masterCategory')(x)\n",
    "    subCategory_output = Dense(num_classes['subCategory'], activation='softmax', name='subCategory')(x)\n",
    "    articleType_output = Dense(num_classes['articleType'], activation='softmax', name='articleType')(x)\n",
    "    baseColour_output = Dense(num_classes['baseColour'], activation='softmax', name='baseColour')(x)\n",
    "    season_output = Dense(num_classes['season'], activation='softmax', name='season')(x)\n",
    "    usage_output = Dense(num_classes['usage'], activation='softmax', name='usage')(x)\n",
    "\n",
    "    model = Model(inputs=base_model.input, \n",
    "                  outputs=[gender_output, masterCategory_output, subCategory_output, articleType_output, \n",
    "                           baseColour_output, season_output, usage_output])\n",
    "\n",
    "    # Tune learning rate\n",
    "    learning_rate = hp.Float('learning_rate', min_value=1e-5, max_value=1e-3, sampling='log')\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss={\n",
    "            'articleType': 'categorical_crossentropy',\n",
    "            'baseColour': 'categorical_crossentropy',\n",
    "            'gender': 'categorical_crossentropy',\n",
    "            'masterCategory': 'categorical_crossentropy',\n",
    "            'season': 'categorical_crossentropy',\n",
    "            'subCategory': 'categorical_crossentropy',\n",
    "            'usage': 'categorical_crossentropy'\n",
    "        },\n",
    "        metrics={\n",
    "            'articleType': 'accuracy',\n",
    "            'baseColour': 'accuracy',\n",
    "            'gender': 'accuracy',\n",
    "            'masterCategory': 'accuracy',\n",
    "            'season': 'accuracy',\n",
    "            'subCategory': 'accuracy',\n",
    "            'usage': 'accuracy'\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f61d508",
   "metadata": {},
   "outputs": [],
   "source": [
    "## to delete tuner directory if needed\n",
    "# import shutil\n",
    "\n",
    "# tuner_dir = 'fine_tune_tuner_results'\n",
    "\n",
    "# if os.path.exists(tuner_dir):\n",
    "#     shutil.rmtree(tuner_dir)\n",
    "#     print(f\"Directory '{tuner_dir}' has been deleted.\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4110cd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the tuner with a specific objective\n",
    "tuner = kt.RandomSearch(\n",
    "    build_model,\n",
    "    objective=kt.Objective(\"articleType_accuracy\", direction=\"max\"),\n",
    "    max_trials=10,           # minimize this to test .............................................................\n",
    "    executions_per_trial=1,\n",
    "    directory='tuner_results',\n",
    "    project_name='Labbesni'\n",
    ")\n",
    "\n",
    "# Display search space summary\n",
    "tuner.search_space_summary()\n",
    "\n",
    "# Define callbacks\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    filepath='best_model_epoch_{epoch:02d}_articleType_acc_{articleType_accuracy:.2f}.keras',  # Save the best model based on validation accuracy\n",
    "    monitor='articleType_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "reduce_lr_callback = ReduceLROnPlateau(\n",
    "    monitor='articleType_accuracy',\n",
    "    mode='max',\n",
    "    factor=0.2,\n",
    "    patience=3,  # Number of epochs with no improvement after which the learning rate will be reduced\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "early_stopping_callback = EarlyStopping(\n",
    "    monitor='articleType_accuracy',\n",
    "    mode='max',\n",
    "    patience=5,  # Number of epochs with no improvement after which training will stop\n",
    "    verbose=1,\n",
    "    restore_best_weights=True  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89611a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the hyperparameter search\n",
    "try:\n",
    "    tuner.search(\n",
    "        train_dataset.repeat(), \n",
    "        validation_data=val_dataset, \n",
    "        epochs=25,                    # minimize this to test .....................................................\n",
    "        steps_per_epoch=steps_per_epoch, \n",
    "        validation_steps=validation_steps,\n",
    "        callbacks=[checkpoint_callback, reduce_lr_callback, early_stopping_callback]\n",
    "    )\n",
    "except Exception as e:\n",
    "        print(f\"Error during hyperparameter search: {e}\")\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(f\"Best number of units in dense layers: {best_hps.get('units')}\")\n",
    "print(f\"Best learning rate: {best_hps.get('learning_rate')}\")\n",
    "\n",
    "# Build the model with the best hyperparameters and train it\n",
    "model = tuner.hypermodel.build(best_hps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42369af",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    history = model.fit(\n",
    "        train_dataset.repeat(),\n",
    "        validation_data=val_dataset,\n",
    "        #initial_epoch=, # if the training stop at any epoch write the number of this epoch to continue training from this point\n",
    "        epochs=30,                  # minimize this to test ...............................................\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        validation_steps=validation_steps,\n",
    "        callbacks=[checkpoint_callback, reduce_lr_callback, early_stopping_callback]\n",
    "    )\n",
    "except Exception as e:\n",
    "    logging.error(f\"An error occurred during training: {e}\")\n",
    "    \n",
    "model.save('best_initial_model.keras') \n",
    "print(\"Model saved successfully.\") \n",
    "\n",
    "with open('history.pkl', 'wb') as f:\n",
    "    pickle.dump(history.history, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf124b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = tf.keras.models.load_model('best_initial_model.keras')\n",
    "# with open('history.pkl', 'rb') as f:\n",
    "#     history = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f77ed24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_fine_tune_model(hp):\n",
    "    try:\n",
    "        model = tf.keras.models.load_model('best_initial_model.keras')\n",
    "    except OSError as e:\n",
    "        print(f\"Error: {e}. Model file not found or corrupted. Please check the path or model format.\")\n",
    "        return None  s\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error while loading the model: {e}\")\n",
    "        return None  \n",
    "        # Freeze all layers initially\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    # Determine the number of layers to unfreeze\n",
    "    total_layers = len(model.layers)\n",
    "    unfreeze_layers = hp.Int('unfreeze_layers', min_value=10, max_value=total_layers, step=20)\n",
    "    \n",
    "    # Unfreeze the specified number of layers from the top\n",
    "    for layer in model.layers[-unfreeze_layers:]:\n",
    "        layer.trainable = True\n",
    "        \n",
    "    fine_tune_lr = hp.Float('fine_tune_learning_rate', min_value=1e-6, max_value=1e-4, sampling='log')\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=fine_tune_lr),\n",
    "        loss={\n",
    "            'articleType': 'categorical_crossentropy',\n",
    "            'baseColour': 'categorical_crossentropy',\n",
    "            'gender': 'categorical_crossentropy',\n",
    "            'masterCategory': 'categorical_crossentropy',\n",
    "            'season': 'categorical_crossentropy',\n",
    "            'subCategory': 'categorical_crossentropy',\n",
    "            'usage': 'categorical_crossentropy'\n",
    "        },\n",
    "        metrics={\n",
    "            'articleType': 'accuracy',\n",
    "            'baseColour': 'accuracy',\n",
    "            'gender': 'accuracy',\n",
    "            'masterCategory': 'accuracy',\n",
    "            'season': 'accuracy',\n",
    "            'subCategory': 'accuracy',\n",
    "            'usage': 'accuracy'\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Initialize the tuner for fine-tuning\n",
    "fine_tune_tuner = kt.RandomSearch(\n",
    "    build_fine_tune_model,\n",
    "    objective=kt.Objective(\"articleType_accuracy\", direction=\"max\"),\n",
    "    max_trials=10,            # minimize this to test ...............................................\n",
    "    executions_per_trial=1,\n",
    "    directory='fine_tune_tuner_results',\n",
    "    project_name='Labbesni_fine_tuning'\n",
    ")\n",
    "\n",
    "fine_tune_tuner.search_space_summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501f05f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define checkpoint callback to save the best model during fine-tuning\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    filepath='best_fine_tuned_model.keras',\n",
    "    monitor='articleType_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "\n",
    "lr_scheduler = ReduceLROnPlateau(\n",
    "    monitor='articleType_accuracy',\n",
    "    mode='max',\n",
    "    factor=0.2,\n",
    "    patience=3,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='articleType_accuracy',\n",
    "    mode='max',\n",
    "    patience=5,\n",
    "    verbose=1,\n",
    "    restore_best_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504b0fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the hyperparameter search for fine-tuning\n",
    "try:\n",
    "    fine_tune_tuner.search(\n",
    "        train_dataset, \n",
    "        validation_data=val_dataset, \n",
    "        epochs=25,                # minimize this to test ...............................................\n",
    "        steps_per_epoch=steps_per_epoch, \n",
    "        validation_steps=validation_steps,\n",
    "        callbacks=[checkpoint_callback, lr_scheduler, early_stopping]\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"Error during hyperparameter search: {e}\")\n",
    "\n",
    "best_hps_fine_tune = fine_tune_tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "\n",
    "print(f\"Best number of unfreeze layers: {best_hps_fine_tune.get('unfreeze_layers')}\")\n",
    "print(f\"Best fine-tune learning rate: {best_hps_fine_tune.get('fine_tune_learning_rate')}\")\n",
    "\n",
    "# Unfreeze layers and fine-tune with the best hyperparameters\n",
    "model = fine_tune_tuner.hypermodel.build(best_hps_fine_tune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe64608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tune the model\n",
    "try:\n",
    "    history_fine_tuning = model.fit(\n",
    "        train_dataset.repeat(),\n",
    "        validation_data=val_dataset,\n",
    "        #initial_epoch=,\n",
    "        epochs=30,\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        validation_steps=validation_steps,\n",
    "        callbacks=[checkpoint_callback, lr_scheduler, early_stopping]\n",
    "    )\n",
    "except Exception as e:\n",
    "    logging.error(f\"An error occurred during training: {e}\")\n",
    "    \n",
    "model.save('tuned_model.keras')\n",
    "print(\"Model saved successfully.\")     \n",
    "\n",
    "import pickle\n",
    "with open('history_fine_tuning.pkl', 'wb') as f:\n",
    "    pickle.dump(history_fine_tuning.history, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae96b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = tf.keras.models.load_model('tuned_model.keras')\n",
    "# with open('history_fine_tuning.pkl', 'rb') as f:\n",
    "#     history_fine_tuning = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109ee870",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.evaluate(val_dataset, steps=validation_steps)\n",
    "print(f\"Evaluation results: {results}\")\n",
    "with open('evaluation_results.pkl', 'wb') as f:\n",
    "    pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8262374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('evaluation_results.pkl', 'rb') as f:\n",
    "#     results = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3787d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "total_loss = results[0]\n",
    "\n",
    "# The order of categories\n",
    "categories = ['gender', 'masterCategory', 'subCategory', 'articleType', 'baseColour', 'season', 'usage']\n",
    "\n",
    "# Unpack losses and accuracies based on the known order\n",
    "losses = results[1:8]  # Adjust based on your model's output\n",
    "accuracies = results[8:]\n",
    "\n",
    "# Create a dictionary to store results\n",
    "evaluation_results = {\n",
    "    \"Total Loss\": total_loss,\n",
    "}\n",
    "\n",
    "# Print evaluation results and store them in the dictionary\n",
    "for i, category in enumerate(categories):\n",
    "    loss = losses[i]\n",
    "    accuracy = accuracies[i]\n",
    "    evaluation_results[category] = {\n",
    "        \"Loss\": loss,\n",
    "        \"Accuracy\": accuracy\n",
    "    }\n",
    "    print(f\"{category.capitalize()} Loss: {loss}, Accuracy: {accuracy}\")\n",
    "print(f\"Total Loss: {total_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "06b49601",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('evaluation_results.json', 'w') as json_file:\n",
    "    json.dump(evaluation_results, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01788192",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('evaluation_results.json', 'r') as json_file:\n",
    "    loaded_results = json.load(json_file)\n",
    "\n",
    "# Access the loaded results in the same way\n",
    "print(f\"Total Loss: {loaded_results['Total Loss']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2767f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot training & validation accuracy for each category\n",
    "def plot_accuracy(history, category):\n",
    "    plt.plot(history.history[category + '_accuracy'], label=f'Training {category} Accuracy')\n",
    "    plt.plot(history.history['val_' + category + '_accuracy'], label=f'Validation {category} Accuracy')\n",
    "    plt.title(f'{category} Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "categories = ['gender', 'masterCategory', 'subCategory', 'articleType', 'baseColour', 'season', 'usage']\n",
    "\n",
    "for category in categories:\n",
    "    plot_accuracy(history, category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be83e619",
   "metadata": {},
   "outputs": [],
   "source": [
    "for category in categories:\n",
    "    plot_accuracy(history_fine_tuning, category)\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"An error occurred during training: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3858d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Available keys in history.history:\")\n",
    "# print(history.history.keys())\n",
    "\n",
    "# # Plotting accuracy for all available accuracies\n",
    "# plt.figure(figsize=(12, 8))\n",
    "\n",
    "# for key in history.history.keys():\n",
    "#     if 'accuracy' in key:  \n",
    "#         plt.plot(history.history[key], label=key)\n",
    "\n",
    "# plt.title('Model Accuracy')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.legend(loc='upper left')\n",
    "# plt.show()\n",
    "\n",
    "# # Plotting loss for training and validation\n",
    "# plt.figure(figsize=(12, 8))\n",
    "\n",
    "# if 'loss' in history.history:\n",
    "#     plt.plot(history.history['loss'], label='Training loss')\n",
    "\n",
    "# if 'val_loss' in history.history:\n",
    "#     plt.plot(history.history['val_loss'], label='Validation loss')\n",
    "\n",
    "# plt.title('Model Loss')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.legend(loc='upper left')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b246d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if history_fine_tuning:\n",
    "#     print(\"Available keys in history_fine_tuning.history:\")\n",
    "#     print(history_fine_tuning.history.keys())\n",
    "\n",
    "#     # Plotting accuracy for all available accuracies\n",
    "#     plt.figure(figsize=(12, 8))\n",
    "\n",
    "#     for key in history_fine_tuning.history.keys():\n",
    "#         if 'accuracy' in key: \n",
    "#             plt.plot(history_fine_tuning.history[key], label=key)\n",
    "\n",
    "#     plt.title('Fine-Tuned Model Accuracy')\n",
    "#     plt.ylabel('Accuracy')\n",
    "#     plt.xlabel('Epoch')\n",
    "#     plt.legend(loc='upper left')\n",
    "#     plt.show()\n",
    "\n",
    "#     # Plotting loss for training and validation\n",
    "#     plt.figure(figsize=(12, 8))\n",
    "\n",
    "#     if 'loss' in history_fine_tuning.history:\n",
    "#         plt.plot(history_fine_tuning.history['loss'], label='Training loss')\n",
    "\n",
    "#     if 'val_loss' in history_fine_tuning.history:\n",
    "#         plt.plot(history_fine_tuning.history['val_loss'], label='Validation loss')\n",
    "\n",
    "#     plt.title('Fine-Tuned Model Loss')\n",
    "#     plt.ylabel('Loss')\n",
    "#     plt.xlabel('Epoch')\n",
    "#     plt.legend(loc='upper left')\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb4ee79",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = {\n",
    "    'gender': ['Boys', 'Girls', 'Men', 'Unisex', 'Women'],\n",
    "    'masterCategory': ['Accessories', 'Apparel', 'Footwear'],\n",
    "    'subCategory': ['Bags', 'Belts', 'Bottomwear', 'Cufflinks', 'Dress', 'Eyewear',\n",
    "       'Flip Flops', 'Headwear', 'Jewellery', 'Sandal', 'Scarves',\n",
    "       'Shoes', 'Socks', 'Stoles', 'Ties', 'Topwear', 'Watches'],\n",
    "    'articleType': ['Backpacks', 'Bangle', 'Belts', 'Blazers', 'Bracelet', 'Capris',\n",
    "       'Caps', 'Casual Shoes', 'Churidar', 'Clutches', 'Cufflinks',\n",
    "       'Dresses', 'Duffel Bag', 'Earrings', 'Flats',\n",
    "       'Flip Flops', 'Formal Shoes', 'Handbags', 'Hat', 'Headband',\n",
    "       'Heels', 'Jackets', 'Jeans', 'Jeggings', 'Jewellery Set',\n",
    "       'Jumpsuit', 'Kurtas', 'Kurtis', 'Laptop Bag', 'Leggings',\n",
    "       'Messenger Bag', 'Mobile Pouch',\n",
    "       'Necklace and Chains', 'Patiala', 'Pendant',\n",
    "       'Rain Jacket', 'Ring', 'Rompers', 'Rucksacks', 'Sandals',\n",
    "       'Scarves', 'Shirts', 'Shorts', 'Skirts', 'Socks',\n",
    "       'Sports Sandals', 'Sports Shoes', 'Stockings', 'Stoles',\n",
    "       'Sunglasses', 'Sweaters', 'Sweatshirts', 'Swimwear',\n",
    "        'Ties', 'Tights', 'Tops',\n",
    "       'Track Pants', 'Tracksuits', 'Travel Accessory', 'Trolley Bag',\n",
    "       'Trousers', 'Tshirts', 'Tunics', 'Waist Pouch', 'Waistcoat',\n",
    "       'Wallets', 'Watches'],\n",
    "    'baseColour': ['Beige', 'Black', 'Blue', 'Bronze', 'Brown', 'Burgundy',\n",
    "       'Charcoal', 'Coffee Brown', 'Copper', 'Cream', 'Fluorescent Green',\n",
    "       'Gold', 'Green', 'Grey', 'Grey Melange', 'Khaki', 'Lavender',\n",
    "       'Lime Green', 'Magenta', 'Maroon', 'Mauve', 'Metallic', 'Multi',\n",
    "       'Mushroom Brown', 'Mustard', 'Navy Blue', 'Nude', 'Off White',\n",
    "       'Olive', 'Orange', 'Peach', 'Pink', 'Purple', 'Red', 'Rose',\n",
    "       'Rust', 'Sea Green', 'Silver', 'Skin', 'Steel', 'Tan', 'Taupe',\n",
    "       'Teal', 'Turquoise Blue', 'Unknown', 'White', 'Yellow'],\n",
    "    'season':['Summer', 'Winter'],\n",
    "    'usage': ['Casual', 'Formal', 'Local', 'Party', 'Sports', 'Travel']\n",
    "}\n",
    "\n",
    "for images, labels in val_dataset.take(1):  \n",
    "    \n",
    "    predictions = model.predict(images)\n",
    "\n",
    "\n",
    "    plt.imshow(images[0])#.numpy().astype(\"uint8\"))\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "    true_labels = {\n",
    "        'gender': np.argmax(labels[0][0]),  \n",
    "        'masterCategory': np.argmax(labels[1][0]),\n",
    "        'subCategory': np.argmax(labels[2][0]),\n",
    "        'articleType': np.argmax(labels[3][0]),\n",
    "        'baseColour': np.argmax(labels[4][0]),\n",
    "        'season': np.argmax(labels[5][0]),\n",
    "        'usage': np.argmax(labels[6][0])\n",
    "    }\n",
    "\n",
    "    pred_labels = {\n",
    "        'gender': np.argmax(predictions[0][0]),  \n",
    "        'masterCategory': np.argmax(predictions[1][0]),\n",
    "        'subCategory': np.argmax(predictions[2][0]),\n",
    "        'articleType': np.argmax(predictions[3][0]),\n",
    "        'baseColour': np.argmax(predictions[4][0]),\n",
    "        'season': np.argmax(predictions[5][0]),\n",
    "        'usage': np.argmax(predictions[6][0])\n",
    "    }\n",
    "\n",
    "    print(\"True Labels:\")\n",
    "    for category, index in true_labels.items():\n",
    "        print(f\"{category}: {class_names[category][index]}\")\n",
    "        \n",
    "    print(\"\\nPredicted Labels:\")\n",
    "    for category, index in pred_labels.items():\n",
    "        print(f\"{category}: {class_names[category][index]}\")\n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
